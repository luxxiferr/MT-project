# MT-project
Adjusting Machine Translation Transformer's parameters for different input length texts for the language pair English-Spanish

This study delves into the influence of input sequence lengths on vector representation quality
in Machine Translation (MT) models, specifically English-Spanish MT. We postulate that
longer sequences could enhance the model’s
predictive abilities due to a deeper contextual understanding and greater interdependence
among input tokens. We assess this hypothesis using three sub-corpora extracted from the
same English-Spanish parallel corpora, categorized by sequence lengths: short (1-3 words),
medium (4-10 words), and long (more than 10
words). We also explore the effect of hyperparameter tuning on the model’s performance,
aiming to strike a balance between accuracy
and practical constraints like training time and
computational resources. The findings provide
insights into optimizing sequence lengths and
parameters in MT models.
